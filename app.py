import os
os.environ["STREAMLIT_SERVER_FILE_WATCHER"] = "none"

import cv2
import streamlit as st
import tempfile
import numpy as np
import time
from threading import Lock
from ultralytics import YOLO

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
camera_lock = Lock()

# –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ü–ï–†–í–´–ú Streamlit-–≤—ã–∑–æ–≤–æ–º
st.set_page_config(page_title="People Detector", layout="wide")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MODEL_PATH = "runs/detect/custom_yolov8n2/weights/best.onnx"
CLASS_NAMES = ["person"]
CONF_THRESH = 0.5
CAMERA_ID = 0

@st.cache_resource
def load_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ ONNX-–º–æ–¥–µ–ª–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    return YOLO(MODEL_PATH, task='detect')

def process_frame(_model, frame):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ —Å –∑–∞–º–µ—Ä–∞–º–∏ –≤—Ä–µ–º–µ–Ω–∏"""
    start_time = time.perf_counter()
    
    # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏
    results = _model.predict(frame, conf=CONF_THRESH, verbose=False)
    inference_time = time.perf_counter() - start_time
    
    # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    res_img = results[0].plot()
    postprocess_time = time.perf_counter() - start_time - inference_time
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤—Ä–µ–º–µ–Ω–∏
    timing_text = f"Inference: {inference_time*1000:.1f}ms | FPS: {1/(inference_time+1e-6):.1f}"
    cv2.putText(res_img, timing_text, (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)
    
    return res_img, len(results[0].boxes)

def process_video(_model, video_path):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º
    output = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False, mode='wb')
    output.close()  # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è VideoWriter
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ VideoWriter —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    writer = cv2.VideoWriter(
        output.name,
        fourcc,
        fps/4,  # –ß–∞—Å—Ç–æ—Ç–∞ –∫–∞–¥—Ä–æ–≤ —Å —É—á–µ—Ç–æ–º –ø—Ä–æ–ø—É—Å–∫–∞
        (frame_width, frame_height)
    )
    
    try:
        frame_count = 0
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        progress_bar = st.progress(0)
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: 
                break
            
            if frame_count % 4 == 0:
                processed, _ = process_frame(_model, frame)
                writer.write(processed)
                
            progress_bar.progress(min(frame_count / total_frames, 1.0))
            frame_count += 1
            
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ: {str(e)}")
    finally:
        cap.release()
        writer.release()
        progress_bar.empty()
    
    # –ü–µ—Ä–µ—á–∏—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª –≤ –±–∏–Ω–∞—Ä–Ω–æ–º —Ä–µ–∂–∏–º–µ
    with open(output.name, 'rb') as f:
        video_bytes = f.read()
    
    os.unlink(output.name)  # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    return video_bytes


def camera_processing():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ —Å –∫–∞–º–µ—Ä—ã"""
    model = load_model()
    cap = None
    
    try:
        with camera_lock:
            cap = cv2.VideoCapture(CAMERA_ID)
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            
            st_frame = st.empty()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–Ω–æ–ø–∫–∏
            if 'camera_running' not in st.session_state:
                st.session_state.camera_running = True
            
            # –°–æ–∑–¥–∞–µ–º –∫–Ω–æ–ø–∫—É –û–î–ò–ù –†–ê–ó —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∫–ª—é—á–æ–º
            stop_button = st.button(
                "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–∞–º–µ—Ä—É",
                key="unique_stop_button_key"
            )
            
            while cap.isOpened() and st.session_state.camera_running:
                success, frame = cap.read()
                if not success:
                    st.error("–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–∞–º–µ—Ä—ã!")
                    break
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞
                processed, count = process_frame(model, frame)
                
                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–¥—Ä–∞
                st_frame.image(processed[:, :, ::-1], 
                             caption=f"–õ—é–¥–µ–π –≤ –∫–∞–¥—Ä–µ: {count}", 
                             use_container_width=True)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —á–µ—Ä–µ–∑ session_state
                if stop_button:
                    st.session_state.camera_running = False
                    break
                
    finally:
        if cap is not None:
            cap.release()
            st.success("–ö–∞–º–µ—Ä–∞ —É—Å–ø–µ—à–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–∞")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = load_model()

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.title("–î–µ—Ç–µ–∫—Ü–∏—è –ª—é–¥–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ üì∑")
st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")

# –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã
app_mode = st.sidebar.selectbox("–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:", 
                               ["–§–æ—Ç–æ/–í–∏–¥–µ–æ", "–í–µ–±-–∫–∞–º–µ—Ä–∞"])

if app_mode == "–í–µ–±-–∫–∞–º–µ—Ä–∞":
    if st.sidebar.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–∞–º–µ—Ä—É"):
        camera_processing()
else:
    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ/–≤–∏–¥–µ–æ
    upload_type = st.sidebar.radio("–¢–∏–ø —Ñ–∞–π–ª–∞:", ["–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "–í–∏–¥–µ–æ"])
    uploaded_file = st.sidebar.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª...", 
                                          type=["jpg", "png", "jpeg", "mp4"])
    
    if uploaded_file:
        if upload_type == "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ":
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
            img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
            
            start_total = time.perf_counter()
            processed, count = process_frame(model, img)
            total_time = time.perf_counter() - start_total
            
            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.image(processed[:, :, ::-1], 
                    use_container_width=True,
                    caption=f"–ù–∞–π–¥–µ–Ω–æ –ª—é–¥–µ–π: {count} | –í—Ä–µ–º—è: {total_time*1000:.1f}ms")
            
        else:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ
            tfile = tempfile.NamedTemporaryFile(delete=False)
            tfile.write(uploaded_file.read())
            
            with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ..."):
                video_bytes = process_video(model, tfile.name)
                st.video(video_bytes)

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
st.sidebar.markdown("---")
st.sidebar.markdown("## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ")
st.sidebar.info(
    f"–ú–æ–¥–µ–ª—å: YOLOv8n (ONNX)\n"
    f"–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {CONF_THRESH}\n"
    f"–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–∞–¥—Ä–∞: 640x480"
)
